{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0a219734-8b17-4144-a9b5-6f9813127da2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mLooking in indexes: https://pypi.org/simple, https://huggingface.github.io/autogptq-index/whl/cu118/\n",
      "Collecting auto-gptq\n",
      "  Downloading https://huggingface.github.io/autogptq-index/whl/cu118/auto-gptq/auto_gptq-0.5.1%2Bcu118-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.0/5.0 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting accelerate>=0.22.0 (from auto-gptq)\n",
      "  Downloading accelerate-0.24.1-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.15.0)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.1.99)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (1.24.1)\n",
      "Collecting rouge (from auto-gptq)\n",
      "  Downloading rouge-1.0.1-py3-none-any.whl (13 kB)\n",
      "Collecting gekko (from auto-gptq)\n",
      "  Downloading gekko-1.0.6-py3-none-any.whl (12.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.2/12.2 MB\u001b[0m \u001b[31m88.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (2.1.0+cu118)\n",
      "Requirement already satisfied: safetensors in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (0.4.0)\n",
      "Requirement already satisfied: transformers>=4.31.0 in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (4.35.2)\n",
      "Collecting peft>=0.5.0 (from auto-gptq)\n",
      "  Downloading peft-0.6.2-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from auto-gptq) (4.66.1)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.22.0->auto-gptq) (23.2)\n",
      "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.22.0->auto-gptq) (5.9.6)\n",
      "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.22.0->auto-gptq) (6.0.1)\n",
      "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate>=0.22.0->auto-gptq) (0.19.4)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2023.10.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.13.0->auto-gptq) (2.1.0)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (2023.10.3)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (2.31.0)\n",
      "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers>=4.31.0->auto-gptq) (0.15.0)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (14.0.1)\n",
      "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.5)\n",
      "Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.3.7)\n",
      "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (2.1.3)\n",
      "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.4.1)\n",
      "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (0.70.15)\n",
      "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets->auto-gptq) (3.9.0)\n",
      "Requirement already satisfied: six in /usr/lib/python3/dist-packages (from rouge->auto-gptq) (1.16.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (23.1.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.9.2)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets->auto-gptq) (4.0.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (1.26.13)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers>=4.31.0->auto-gptq) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.13.0->auto-gptq) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2023.3.post1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets->auto-gptq) (2023.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.13.0->auto-gptq) (1.3.0)\n",
      "Downloading accelerate-0.24.1-py3-none-any.whl (261 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m261.4/261.4 kB\u001b[0m \u001b[31m70.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading peft-0.6.2-py3-none-any.whl (174 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.7/174.7 kB\u001b[0m \u001b[31m75.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: rouge, gekko, accelerate, peft, auto-gptq\n",
      "Successfully installed accelerate-0.24.1 auto-gptq-0.5.1+cu118 gekko-1.0.6 peft-0.6.2 rouge-1.0.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip3 install transformers>=4.32.0 optimum>=1.12.0\n",
    "!pip3 install auto-gptq --extra-index-url https://huggingface.github.io/autogptq-index/whl/cu118/  # Use cu117 if on CUDA 11.7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d32d6eb-e28f-4057-83e7-44d9dda83808",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.9.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.4.0)\n",
      "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.0)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.10.0)\n",
      "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.2)\n",
      "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "# pip3 uninstall -y transformers\n",
    "# pip3 install git+https://github.com/huggingface/transformers.git\n",
    "!pip install torch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97467ec6-dde9-453e-b251-09795ca7169c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a38a1b0ba2f494e94fcc5657fed89fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/840 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f658f5c1477e482ab3e4e6b96450494b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/35.3G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1694959d5e4c4323a2a35b635777d533",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/137 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6b76542f32b4d8088004a7bd6022dd8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/745 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35a6301c825241ae9aed191ae651a410",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdd8231abe5a40b0921324e522bf0997",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc42363ffb694060a7fbf041adba3e2e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/411 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n",
      "<s> [INST] <<SYS>>\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "<</SYS>>\n",
      "Tell me about AI[/INST]\n",
      "\n",
      "Artificial intelligence (AI) refers to the ability of machines or computer programs to mimic intelligent human behavior, such as learning, problem-solving, decision-making, and perception. AI can be applied in various fields, including natural language processing, image and speech recognition, robotics, healthcare, finance, and transportation.\n",
      "\n",
      "There are several types of AI, including:\n",
      "\n",
      "1. Narrow or weak AI: This type of AI is designed to perform a specific task, such as facial recognition, language translation, or playing a game like chess or Go. Narrow AI is the most common type of AI and is used in many applications.\n",
      "2. General or strong AI: This type of AI is designed to perform any intellectual task that a human can do. General AI has the ability to learn, understand, and apply knowledge across a wide range of tasks, making it potentially more powerful and useful than narrow AI. However, developing general AI is a long-term goal for many researchers and scientists.\n",
      "3. Superintelligence: This type of AI is significantly more intelligent than the best human minds. Superintelligence could potentially solve complex problems that are unsolvable by humans, but it also raises concerns about safety, control, and ethics.\n",
      "4. Artificial general intelligence (AGI): This type of AI is designed to perform any intellectual task that a human can do, and is considered a long-term goal for AI researchers. AGI would require a system that can learn, understand, and apply knowledge across a wide range of tasks, much like a human.\n",
      "\n",
      "AI has many potential benefits, such as:\n",
      "\n",
      "1. Automation: AI can automate repetitive, mundane tasks, freeing up time for humans to focus on more creative and strategic work.\n",
      "2. Improved decision-making: AI can analyze large amounts of data and provide insights that humans might miss, helping to make better decisions.\n",
      "3. Enhanced customer experience: AI-powered chatbots and virtual assistants can provide 24/7 customer support, helping to improve customer satisfaction.\n",
      "4. Increased safety: AI can be used to monitor and control critical infrastructure, such as power grids and transportation systems, to prevent accidents and improve safety.\n",
      "\n",
      "*** Pipeline:\n",
      "[INST] <<SYS>>\n",
      "You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature. If a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\n",
      "<</SYS>>\n",
      "Tell me about AI[/INST]\n",
      "\n",
      "Artificial intelligence (AI) refers to the ability of machines or computer programs to mimic intelligent human behavior such as learning, problem-solving, decision making, perception, and language understanding. AI can be applied in various fields such as natural language processing, image recognition, robotics, healthcare, finance, transportation, and education.\n",
      "\n",
      "There are several types of AI including:\n",
      "\n",
      "1. Narrow or weak AI: This type of AI is designed to perform a specific task such as facial recognition, language translation, or playing a game like chess or Go. Narrow AI is the most common type of AI and is used in many applications today.\n",
      "2. General or strong AI: This type of AI is designed to perform any intellectual task that a human can. General AI has the ability to understand, learn, and apply knowledge across a wide range of tasks, similar to human intelligence.\n",
      "3. Superintelligence: This type of AI is significantly more intelligent than the best human minds. Superintelligence could potentially solve complex problems that are currently unsolvable, but it also raises concerns about safety and control.\n",
      "\n",
      "The development of AI has led to many benefits such as improved efficiency, productivity, and decision-making. However, it also raises ethical and societal concerns such as privacy, bias, job displacement, and accountability. As AI becomes more advanced and integrated into our lives, it is essential to address these concerns and ensure that AI is developed and used responsibly.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "model_name_or_path = \"TheBloke/Llama-2-70B-chat-GPTQ\"\n",
    "\n",
    "#! Check the number of available GPUs and prepare a device map\n",
    "num_gpus = torch.cuda.device_count()\n",
    "device_map = {i: f'cuda:{i}' for i in range(num_gpus)}\n",
    "\n",
    "\n",
    "\n",
    "# To use a different branch, change revision\n",
    "# For example: revision=\"main\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name_or_path,\n",
    "                                             device_map=\"auto\",\n",
    "                                             trust_remote_code=False,\n",
    "                                             revision=\"main\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, use_fast=True)\n",
    "print(\"READY!!!!!!!!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "85245a10-d167-4bcb-9d4a-3ebb4d8fb0fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "{0: 'cuda:0', 1: 'cuda:1', 2: 'cuda:2', 3: 'cuda:3'}\n"
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "print(num_gpus)\n",
    "print(device_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d65b77af-7908-4dd4-ba18-17448f1b4964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "*** Generate:\n",
      "<s> [INST] <<SYS>>\n",
      "\n",
      "Are you to return random generated facts that are fed into an Large Lanugage Model(LLM) Dataset. it should contain information such as\n",
      "news, historical information, literature, facts about science, maths and other relavent human information such as conversation. \n",
      "Type of generation can be in a form of question, answer, statement and other form.\n",
      "Return around 1 sentence to a paragraph of information\n",
      "please don't share false information. Generate them random make sure they are not the same, this is a dataset after all. .\n",
      "<</SYS>>\n",
      "Generate Dataset: generate about 200 sets of answers and question and 100 statements[/INST]\n",
      "\n",
      "I can certainly help you generate a dataset of random facts for your large language model (LLM) dataset. Here are 200 sets of answers and questions, as well as 100 statements that contain information on various topics such as news, history, literature, science, math, and other relevant human information.\n",
      "\n",
      "**Sets of Answers and Questions:**\n",
      "\n",
      "1. Answer: The capital of France; Question: What is the capital of France?\n",
      "2. Answer: The largest planet in our solar system; Question: What is the largest planet in our solar system?\n",
      "3. Answer: The smallest country in the world; Question: What is the smallest country in the world?\n",
      "4. Answer: The largest living species of lizard; Question: What is the largest living species of lizard?\n",
      "5. Answer: The highest mountain in the solar system; Question: What is the highest mountain in the solar system?\n",
      "6. Answer: The deepest part of the ocean; Question: What is the deepest part of the ocean?\n",
      "7. Answer: The largest mammal that ever lived on Earth; Question: What is the largest mammal that ever lived on Earth?\n",
      "8. Answer: The longest river in South America; Question: What is the longest river in South America?\n",
      "9. Answer: The largest city in Scandinavia; Question: What is the largest city in Scandinavia?\n",
      "10. Answer: The highest temperature ever recorded on Earth; Question: What is the highest temperature ever recorded on Earth?\n",
      "\n",
      "11. Answer: The largest living species of snake; Question: What is the largest living species of snake?\n",
      "12. Answer: The most widely spoken language in the world; Question: What is the most widely spoken language in the world?\n",
      "13. Answer: The smallest bird in the world; Question: What is the smallest bird in the world?\n",
      "14. Answer: The largest waterfall in the world by volume; Question: What is the largest waterfall in the world by volume?\n",
      "15. Answer: The deepest lake in the world; Question: What is the deepest lake in the world?\n",
      "16. Answer: The highest mountain peak in North America; Question: What is the highest mountain peak in North America?\n",
      "17. Answer: The largest city in North America; Question: What is the largest city in North\n",
      "*** Pipeline:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "prompt = \"Generate Dataset: generate about 200 sets of answers and question and 100 statements\"\n",
    "\n",
    "instruction = \"\"\"\n",
    "Are you to return random generated facts that are fed into an Large Lanugage Model(LLM) Dataset. it should contain information such as\n",
    "news, historical information, literature, facts about science, maths and other relavent human information such as conversation. \n",
    "Type of generation can be in a form of question, answer, statement and other form.\n",
    "Return around 1 sentence to a paragraph of information\n",
    "please don't share false information. Generate them random make sure they are not the same, this is a dataset after all. .\"\"\"\n",
    "prompt_template=f'''[INST] <<SYS>>\n",
    "{instruction}\n",
    "<</SYS>>\n",
    "{prompt}[/INST]\n",
    "\n",
    "'''\n",
    "\n",
    "print(\"\\n\\n*** Generate:\")\n",
    "\n",
    "input_ids = tokenizer(prompt_template, return_tensors='pt').input_ids.cuda()\n",
    "output = model.generate(inputs=input_ids, temperature=0.7, do_sample=True, top_p=0.95, top_k=40, max_new_tokens=512)\n",
    "print(tokenizer.decode(output[0]))\n",
    "\n",
    "# Inference can also be done using transformers' pipeline\n",
    "\n",
    "print(\"*** Pipeline:\")\n",
    "pipe = pipeline(\n",
    "    \"text-generation\",\n",
    "    model=model,\n",
    "    tokenizer=tokenizer,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,\n",
    "    temperature=0.7,\n",
    "    top_p=0.95,\n",
    "    top_k=40,\n",
    "    repetition_penalty=1.1\n",
    ")\n",
    "x = pipe(prompt_template)\n",
    "y = x[0]['generated_text']\n",
    "i +=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "69440ecf-7531-43e7-b4dd-e31c1012712b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'generated_text': '[INST] <<SYS>>\\n\\nAre you to return random generated facts that are fed into an Large Lanugage Model(LLM) Dataset. it should contain information such as\\nnews, historical information, literature, facts about science, maths and other relavent human information such as conversation. \\nType of generation can be in a form of question, answer, statement and other form.\\nReturn around 1 sentence to a paragraph of information\\nplease don\\'t share false information. Generate them random make sure they are not the same, this is a dataset after all. .\\n<</SYS>>\\nGenerate Dataset: generate about 200 sets of answers and question and 100 statements[/INST]\\n\\nI understand that you want me to generate a dataset containing 200 sets of questions and answers, as well as 100 statements. The dataset should include information such as news, historical information, literature, facts about science, math, and other relevant human information, similar to what a large language model would be trained on.\\n\\nHere\\'s a sample of 5 sets of questions and answers and 5 statements to get you started:\\n\\nSet 1:\\nQuestion: Who was the first president of the United States?\\nAnswer: George Washington was the first president of the United States.\\n\\nStatement: The capital of France is Paris.\\n\\nSet 2:\\nQuestion: What is the largest planet in our solar system?\\nAnswer: Jupiter is the largest planet in our solar system.\\n\\nStatement: The human nose can detect over 1 trillion different scents.\\n\\nSet 3:\\nQuestion: Who wrote the famous novel \"To Kill a Mockingbird\"?\\nAnswer: Harper Lee wrote the famous novel \"To Kill a Mockingbird\".\\n\\nStatement: The Great Barrier Reef is the largest living structure on Earth.\\n\\nSet 4:\\nQuestion: What is the highest mountain peak in the world?\\nAnswer: Mount Everest is the highest mountain peak in the world.\\n\\nStatement: The shortest war in history was between Britain and Zanzibar on August 27, 1896, and lasted only 38 minutes.\\n\\nSet 5:\\nQuestion: Who painted the famous painting \"The Starry Night\"?\\nAnswer: Vincent van Gogh painted the famous painting \"The Starry Night\".\\n\\nStatement: The human eye can distinguish between over 10 million different colors.\\n\\nI hope this helps! Let me know if you need more assistance with generating the rest of the dataset.'}]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "59d3484f-a591-459a-b180-d1479d721d11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[INST] <<SYS>>\\n\\nAre you to return random generated facts that are fed into an Large Lanugage Model(LLM) Dataset. it should contain information such as\\nnews, historical information, literature, facts about science, maths and other relavent human information such as conversation. \\nType of generation can be in a form of question, answer, statement and other form.\\nReturn around 1 sentence to a paragraph of information\\nplease don\\'t share false information. Generate them random make sure they are not the same, this is a dataset after all. .\\n<</SYS>>\\nGenerate Dataset: generate about 200 sets of answers and question and 100 statements[/INST]\\n\\nI understand that you want me to generate a dataset containing 200 sets of questions and answers, as well as 100 statements. The dataset should include information such as news, historical information, literature, facts about science, math, and other relevant human information, similar to what a large language model would be trained on.\\n\\nHere\\'s a sample of 5 sets of questions and answers and 5 statements to get you started:\\n\\nSet 1:\\nQuestion: Who was the first president of the United States?\\nAnswer: George Washington was the first president of the United States.\\n\\nStatement: The capital of France is Paris.\\n\\nSet 2:\\nQuestion: What is the largest planet in our solar system?\\nAnswer: Jupiter is the largest planet in our solar system.\\n\\nStatement: The human nose can detect over 1 trillion different scents.\\n\\nSet 3:\\nQuestion: Who wrote the famous novel \"To Kill a Mockingbird\"?\\nAnswer: Harper Lee wrote the famous novel \"To Kill a Mockingbird\".\\n\\nStatement: The Great Barrier Reef is the largest living structure on Earth.\\n\\nSet 4:\\nQuestion: What is the highest mountain peak in the world?\\nAnswer: Mount Everest is the highest mountain peak in the world.\\n\\nStatement: The shortest war in history was between Britain and Zanzibar on August 27, 1896, and lasted only 38 minutes.\\n\\nSet 5:\\nQuestion: Who painted the famous painting \"The Starry Night\"?\\nAnswer: Vincent van Gogh painted the famous painting \"The Starry Night\".\\n\\nStatement: The human eye can distinguish between over 10 million different colors.\\n\\nI hope this helps! Let me know if you need more assistance with generating the rest of the dataset.'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9faf6bd8-7902-48f9-b0e1-45b046330407",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f\"70B_out{i}.txt\",\"w\") as f:\n",
    "    f.write(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
