Training Llama v2 model with params:
base_model: openthaigpt/openthaigpt-1.0.0-beta-7b-chat-ckpt-hf
data_path: data/
output_dir: ./synthetic_v2_fast
batch_size: 128
micro_batch_size: 4
num_epochs: 3
learning_rate: 0.0003
cutoff_len: 256
val_set_size: 2000
lora_r: 8
lora_alpha: 16
lora_dropout: 0.05
lora_target_modules: ['q_proj', 'v_proj']
train_on_inputs: True
add_eos_token: False
group_by_length: False
wandb_project: 
wandb_run_name: 
wandb_watch: 
wandb_log_model: 
resume_from_checkpoint: False
prompt template: llama_v2

